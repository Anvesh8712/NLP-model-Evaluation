# -*- coding: utf-8 -*-
"""articleSelect.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ud-OLmUa2BSGXifFVdPIjssec4NXG2fj
"""

!pip install benzinga
!pip install BeautifulSoup
!pip install yfinance
!pip install pandas

import pandas as pd

def process_ticker_data(file_path):
    # Load the spreadsheet
    # Assuming the ticker symbols are in the first column
    # Adjust 'usecols' if the ticker symbols are in a different column
    data = pd.read_excel(file_path, usecols=[5])

    # Assuming the first row is the header and the ticker symbols are in the first column
    ticker_column = data.columns[0]

    # Count the frequency of each ticker
    ticker_counts = data[ticker_column].value_counts()

    # Sort the tickers by frequency in descending order
    sorted_tickers = ticker_counts.sort_values(ascending=False)

    return sorted_tickers

# Replace 'your_spreadsheet.xlsx' with the path to your spreadsheet file
file_path = '/content/raw_analyst_ratings.xlsx'
sorted_tickers = process_ticker_data(file_path)

# Print the sorted list of tickers
print(sorted_tickers[0:100])

import pandas as pd

def process_ticker_data(file_path, output_file):
    # Load the spreadsheet
    data = pd.read_excel(file_path, usecols=[5])

    # Assuming the ticker symbols are in the specified column
    ticker_column = data.columns[0]

    # Count the frequency of each ticker
    ticker_counts = data[ticker_column].value_counts()

    # Sort the tickers by frequency in descending order
    sorted_tickers = ticker_counts.sort_values(ascending=False)

    # Output the top 100 sorted tickers to a file
    sorted_tickers.head(500).to_csv(output_file, header=True)

    print(f"Top 100 tickers have been saved to {output_file}")

# Replace 'your_spreadsheet.xlsx' with the path to your spreadsheet file
file_path = '/content/raw_analyst_ratings.xlsx'
output_file = '/content/sorted_tickers.csv'  # The file where the sorted tickers will be saved
process_ticker_data(file_path, output_file)

from benzinga import news_data
from bs4 import BeautifulSoup
import re
from datetime import datetime, timedelta
import yfinance as yf
import pandas as pd
from datetime import datetime, timedelta
from email.utils import parsedate

# Load the Excel file into a DataFrame
file_path = '/content/raw_analyst_ratings.csv'  # Change to your file's path
df = pd.read_csv(file_path)

# Initialize the API and other variables
api_key = "7de27f42c1cd40a3b5c096550d984524"  # Replace with your actual API key
news_api = news_data.News(api_key)

names2 = {
    'MRK': {'Merck & Co.'},
    'AMZN': {'Amazon.com', 'Amazon'},
    'MS': {'Morgan Stanley'},
    'MU': {'Micron Technology', 'Micron'},
    'NVDA': {'NVIDIA'},
    'QQQ': {'Invesco QQQ Trust', 'Invesco'},
    'M': {'Macy’s'},
    'EBAY': {'eBay'},
    'NFLX': {'Netflix'},
    'GILD': {'Gilead Sciences', 'Gilead'},
    'DAL': {'Delta Air Lines', 'Delta'},
    'JNJ': {'Johnson & Johnson'},
    'QCOM': {'Qualcomm'},
    'BABA': {'Alibaba Group', 'Alibaba'},
    'KO': {'The Coca-Cola Company', 'Coca-Cola'},
    'ORCL': {'Oracle Corporation', 'Oracle'},
    'FDX': {'FedEx Corporation', 'FedEx'},
    'HD': {'The Home Depot', 'Home Depot'},
    'BB': {'BlackBerry'},
    'BMY': {'Bristol Myers Squibb', 'Myers Squibb', 'Bristol-Myers Squibb', 'Bristol-Myers'},
    'JCP': {'J.C. Penney', 'JC Penney', 'J C Penney'},
    'LLY': {'Eli Lilly and Company', 'Lilly', 'Eli Lilly'},
    'CMG': {'Chipotle Mexican Grill', 'Chipotle'},
    'CAT': {'Caterpillar Inc.', 'Caterpillar'},
    'GPRO': {'GoPro'},
    'CHK': {'Chesapeake Energy', 'Chesapeake'},
    'FSLR': {'First Solar'},
    'NOK': {'Nokia'},
    'P': {'Pandora Media', 'Pandora'},
    'LMT': {'Lockheed Martin', 'Lockheed'},
    'MCD': {'McDonald’s'},
    'MA': {'Mastercard'},
    'EA': {'Electronic Arts', 'EA'},
    'FCX': {'Freeport-McMoRan'},
    'GPS': {'Gap Inc.', 'Gap'},
    'PEP': {'PepsiCo'},
    'GRPN': {'Groupon'},
    'HAL': {'Halliburton'},
    'LOW': {"Lowe's", 'Lowe’s Companies'},
    'ADBE': {'Adobe Inc.', 'Adobe'},
    'AZN': {'AstraZeneca'},
    'MYL': {'Mylan'},
    'DISH': {'Dish Network', 'Dish'},
    'ATVI': {'Activision Blizzard', 'Activision'},
    'MDT': {'Medtronic'},
    'DB': {'Deutsche Bank', 'Deutsche'},
    'LNKD': {'LinkedIn'}
}

names3 = {
    'MRK': {'Merck & Co.'},
    'AMZN': {'Amazon.com', 'Amazon'},
    'MS': {'Morgan Stanley'},
    'MU': {'Micron Technology', 'Micron'},
    'NVDA': {'NVIDIA'},
    'QQQ': {'Invesco QQQ Trust', 'Invesco'},
    'M': {'Macy’s'},
    'EBAY': {'eBay'},
    'NFLX': {'Netflix'},
    'GILD': {'Gilead Sciences', 'Gilead'},
    'DAL': {'Delta Air Lines', 'Delta'},
    'JNJ': {'Johnson & Johnson'},
    'QCOM': {'Qualcomm'},
    'BABA': {'Alibaba Group', 'Alibaba'},
    'KO': {'The Coca-Cola Company', 'Coca-Cola'},
    'ORCL': {'Oracle Corporation', 'Oracle'},
    'FDX': {'FedEx Corporation', 'FedEx'},
    'HD': {'The Home Depot', 'Home Depot'},
    'BB': {'BlackBerry'},
    'BMY': {'Bristol Myers Squibb', 'Myers Squibb', 'Bristol-Myers Squibb', 'Bristol-Myers'},
    'JCP': {'J.C. Penney', 'JC Penney', 'J C Penney'},
    'LLY': {'Eli Lilly and Company', 'Lilly', 'Eli Lilly'},
    'CMG': {'Chipotle Mexican Grill', 'Chipotle'},
    'CAT': {'Caterpillar Inc.', 'Caterpillar'},
    'GPRO': {'GoPro'},
    'CHK': {'Chesapeake Energy', 'Chesapeake'},
    'FSLR': {'First Solar'},
    'NOK': {'Nokia'},
    'P': {'Pandora Media', 'Pandora'},
    'LMT': {'Lockheed Martin', 'Lockheed'},
    'MCD': {'McDonald’s'},
    'MA': {'Mastercard'},
    'EA': {'Electronic Arts', 'EA'},
    'FCX': {'Freeport-McMoRan'},
    'GPS': {'Gap Inc.', 'Gap'},
    'PEP': {'PepsiCo'},
    'GRPN': {'Groupon'},
    'HAL': {'Halliburton'},
    'LOW': {"Lowe's", 'Lowe’s Companies'},
    'ADBE': {'Adobe Inc.', 'Adobe'},
    'AZN': {'AstraZeneca'},
    'MYL': {'Mylan'},
    'DISH': {'Dish Network', 'Dish'},
    'ATVI': {'Activision Blizzard', 'Activision'},
    'MDT': {'Medtronic'},
    'DB': {'Deutsche Bank', 'Deutsche'},
    'LNKD': {'LinkedIn'},
    # Added entries
    'AA': {'Alcoa Corporation', 'Alcoa'},
    'EWU': {'iShares MSCI United Kingdom ETF', 'EWU ETF'},
    'AGN': {'Allergan', 'Allergan PLC'},
    'EWJ': {'iShares MSCI Japan ETF', 'EWJ ETF'},
    'GLD': {'SPDR Gold Trust', 'GLD ETF'},
    'EWP': {'iShares MSCI Spain ETF', 'EWP ETF'},
    'EWC': {'iShares MSCI Canada ETF', 'EWC ETF'},
    'APC': {'Anadarko Petroleum', 'Anadarko'},
    'AVGO': {'Broadcom Inc.', 'Broadcom'},
    'PCLN': {'Booking Holdings', 'Booking.com'},
    'AIG': {'American International Group', 'AIG'},
    'EWZ': {'iShares MSCI Brazil ETF', 'EWZ ETF'},
    'GOOGL': {'Alphabet Inc.', 'Google'},
    'CCL': {'Carnival Corporation', 'Carnival Cruise'},
    'HUM': {'Humana Inc.', 'Humana'},
    'FCAU': {'Fiat Chrysler Automobiles', 'Fiat Chrysler'},
    'DD': {'DuPont de Nemours, Inc.', 'DuPont'},
    'CRM': {'Salesforce.com, Inc.', 'Salesforce'},
    'MMM': {'3M Company', '3M'}
}

names4 = {
    'MRK': ['Merck & Co.'],
    'AMZN': ['Amazon.com', 'Amazon'],
    'MS': ['Morgan Stanley'],
    'MU': ['Micron Technology', 'Micron'],
    'NVDA': ['NVIDIA'],
    'QQQ': ['Invesco QQQ Trust', 'Invesco'],
    'M': ['Macy’s'],
    'EBAY': ['eBay'],
    'NFLX': ['Netflix'],
    'GILD': ['Gilead Sciences', 'Gilead'],
    'DAL': ['Delta Air Lines', 'Delta'],
    'JNJ': ['Johnson & Johnson'],
    'QCOM': ['Qualcomm'],
    'BABA': ['Alibaba Group', 'Alibaba'],
    'KO': ['The Coca-Cola Company', 'Coca-Cola'],
    'ORCL': ['Oracle Corporation', 'Oracle'],
    'FDX': ['FedEx Corporation', 'FedEx'],
    'HD': ['The Home Depot', 'Home Depot'],
    'BB': ['BlackBerry'],
    'BMY': ['Bristol Myers Squibb', 'Myers Squibb', 'Bristol-Myers Squibb', 'Bristol-Myers'],
    'JCP': ['J.C. Penney', 'JC Penney', 'J C Penney'],
    'LLY': ['Eli Lilly and Company', 'Lilly', 'Eli Lilly'],
    'CMG': ['Chipotle Mexican Grill', 'Chipotle'],
    'CAT': ['Caterpillar Inc.', 'Caterpillar'],
    'GPRO': ['GoPro'],
    'CHK': ['Chesapeake Energy', 'Chesapeake'],
    'FSLR': ['First Solar'],
    'NOK': ['Nokia'],
    'P': ['Pandora Media', 'Pandora'],
    'LMT': ['Lockheed Martin', 'Lockheed'],
    'MCD': ['McDonald’s'],
    'MA': ['Mastercard'],
    'EA': ['Electronic Arts', 'EA'],
    'FCX': ['Freeport-McMoRan'],
    'GPS': ['Gap Inc.', 'Gap'],
    'PEP': ['PepsiCo'],
    'GRPN': ['Groupon'],
    'HAL': ['Halliburton'],
    'LOW': ["Lowe's", 'Lowe’s Companies'],
    'ADBE': ['Adobe Inc.', 'Adobe'],
    'AZN': ['AstraZeneca'],
    'MYL': ['Mylan'],
    'DISH': ['Dish Network', 'Dish'],
    'ATVI': ['Activision Blizzard', 'Activision'],
    'MDT': ['Medtronic'],
    'DB': ['Deutsche Bank', 'Deutsche'],
    'LNKD': ['LinkedIn'],
    'AA': ['Alcoa Corporation', 'Alcoa'],
    'EWU': ['iShares MSCI United Kingdom ETF', 'EWU ETF'],
    'AGN': ['Allergan', 'Allergan PLC'],
    'EWJ': ['iShares MSCI Japan ETF', 'EWJ ETF'],
    'GLD': ['SPDR Gold Trust', 'GLD ETF'],
    'EWP': ['iShares MSCI Spain ETF', 'EWP ETF'],
    'EWC': ['iShares MSCI Canada ETF', 'EWC ETF'],
    'APC': ['Anadarko Petroleum', 'Anadarko'],
    'AVGO': ['Broadcom Inc.', 'Broadcom'],
    'PCLN': ['Booking Holdings', 'Booking.com'],
    'AIG': ['American International Group', 'AIG'],
    'EWZ': ['iShares MSCI Brazil ETF', 'EWZ ETF'],
    'GOOGL': ['Alphabet Inc.', 'Google'],
    'CCL': ['Carnival Corporation', 'Carnival Cruise'],
    'HUM': ['Humana Inc.', 'Humana'],
    'FCAU': ['Fiat Chrysler Automobiles', 'Fiat Chrysler'],
    'DD': ['DuPont de Nemours, Inc.', 'DuPont'],
    'CRM': ['Salesforce.com, Inc.', 'Salesforce'],
    'MMM': ['3M Company', '3M'],
    'BBRY': ['BlackBerry', 'BlackBerry Limited'],
    'BIIB': ['Biogen Inc.', 'Biogen'],
    'EWI': ['EWI ETF', 'iShares MSCI Italy ETF'],
    'BIDU': ['Baidu', 'Baidu, Inc.'],
    'DE': ['Deere & Company', 'John Deere'],
    'AXP': ['American Express Company', 'American Express'],
    'CMCSA': ['Comcast Corporation', 'Comcast'],
    'CVS': ['CVS Health Corporation', 'CVS'],
    'PFE': ['Pfizer Inc.', 'Pfizer'],
    'KR': ['The Kroger Co.', 'Kroger'],
    # ... Additional entries continue here
}

from google.colab import drive
drive.mount('/content/drive')

from bs4 import BeautifulSoup
import yfinance as yf
import csv
from datetime import datetime, timedelta
import string
import re

# Assuming news_api is already defined and initialized


def clean_html(content):
    # Use BeautifulSoup to remove HTML tags
    soup = BeautifulSoup(content, 'html.parser')
    text = soup.get_text()

    # Remove extra whitespaces, tabs, and newlines
    text = text.strip()  # Removes leading and trailing whitespaces
    text = re.sub(r'\s+', ' ', text)  # Replaces multiple whitespace characters with a single space

    text = text.translate(str.maketrans('', '', string.punctuation))

    return text

def get_next_market_dates(created_str, article_date):
    isFri = False
    isSat = False
    isSun = False
    if "Fri" in created_str : isFri = True # Friday
    elif "Sat" in created_str : isSat = True  # Saturday
    elif "Sun" in created_str : isSun = True  # Sunday

    if isFri:
        first_day = article_date  # Use Friday's date for the first price
        second_day = article_date + timedelta(days=3)  # Use Monday's date for the second price
        third_day = article_date + timedelta(days=4)
    elif isSat:
        first_day = article_date - timedelta(days=1)  # Previous Friday
        second_day = article_date + timedelta(days=2)  # Next Monday
        third_day = article_date + timedelta(days=3)
    elif isSun:
        first_day = article_date - timedelta(days=2)  # Previous Friday
        second_day = article_date + timedelta(days=1)  # Next Monday
        third_day = article_date + timedelta(days=2)
    else:
        first_day = article_date  # Current day
        second_day = article_date + timedelta(days=1)
        third_day = article_date + timedelta(days=2)

    return first_day.strftime('%Y-%m-%d'), second_day.strftime('%Y-%m-%d'), third_day.strftime('%Y-%m-%d')


def process_articles(df, names4):
  output_df = pd.DataFrame(columns=['headline', 'article_date', 'ticker', 'open_price_day', 'open_price_next_day', 'article_body','label'])

  for index, row in df.iterrows():
      ticker = row['stock']
      if ticker in names4 and ticker[0] in ('N', 'O', 'P'):
          headline = row['headline']
          article_date_str = row['date']

          try:
              article_date = datetime.strptime(article_date_str, '%Y-%m-%d %H:%M:%S')
          except ValueError:
              print(f"Error parsing date: {article_date_str}")
              continue
          # Fetch articles
          stories = news_api.news(None, None, 'full', None, article_date.strftime('%Y-%m-%d'), article_date.strftime('%Y-%m-%d'), None, None, None, ticker, None)
          if stories:
              for story in stories:
                  if any(name.lower() in story['title'].lower() for name in names4[ticker]):
                      if headline.lower() == story['title'].lower():
                          created_str = story['created']
                          first_day_str, second_day_str, third_day_str = get_next_market_dates(created_str, article_date)
                          stock_data_day = yf.download(ticker, start=first_day_str, end=second_day_str)
                          stock_data_next_day = yf.download(ticker, start=second_day_str, end=third_day_str)

                          open_price_day = stock_data_day['Open'][0] if not stock_data_day.empty else None
                          open_price_next_day = stock_data_next_day['Open'][0] if not stock_data_next_day.empty else None
                          if not open_price_next_day or not open_price_day:
                            continue
                          article_body = clean_html(story.get('body', ''))
                          if not article_body:
                            continue
                          change = ((open_price_next_day - open_price_day)/open_price_day)*100
                          label = ""
                          if abs(change) <= 0.5:
                            label = "N"
                          elif change>0.5:
                            label = "I"
                          else:
                            label = "D"
                          new_row = {
                              'headline': headline,
                              'article_date': article_date.strftime('%Y-%m-%d'),
                              'ticker': ticker,
                              'open_price_day': open_price_day,
                              'open_price_next_day': open_price_next_day,
                              'article_body': article_body,
                              'label': label
                          }
                          output_df = output_df.append(new_row, ignore_index=True)
                          # print("Article processed and added to CSV.")  # Debug print
                          break  # Stop if the matching article is found
                      else:
                          print("")
                          # print(f"No matching story found for headline: {headline}")
  return output_df

# Call the function with the DataFrame, the names dictionary, and the output file name
output_df = process_articles(df, names4)
output_df.to_csv('/content/drive/MyDrive/output_more_NOP.csv', encoding='utf-8', index=False, na_rep='NaN', quotechar='"', quoting=csv.QUOTE_NONNUMERIC)

#CNN MODEL
import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout
from tensorflow.keras.regularizers import l2
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder

# Load the CSV file
file_path = '/content/finaldata.csv'  # Replace with your CSV file path
data = pd.read_csv(file_path)

# Data Preprocessing
# data = data.dropna(subset=['article_body', 'open_price_next_day'])
# data['price_change'] = ((data['open_price_next_day'] - data['open_price_day']) / data['open_price_day']) * 100
# data['stock_movement'] = data['price_change'].apply(lambda x: 'Neutral' if abs(x) < 1 else ('Increase' if x > 0 else 'Decrease'))
def assign_label(open_price_day, open_price_next_day):
    # Calculate percentage change
    percent_change = ((open_price_next_day - open_price_day) / open_price_day) * 100

    # Assign labels based on the criteria
    if percent_change > 1:
        return 'I'  # Increase
    elif percent_change < -1:
        return 'D'  # Decrease
    else:
        return 'N'  # Neutral

data['label'] = data.apply(lambda row: assign_label(row['open_price_day'], row['open_price_next_day']), axis=1)

# Text Processing
tokenizer = Tokenizer()
tokenizer.fit_on_texts(data['article_body'])
sequences = tokenizer.texts_to_sequences(data['article_body'])
max_length = max(len(x) for x in sequences)
padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')

# Label Encoding
label_encoder = LabelEncoder()
encoded_labels = label_encoder.fit_transform(data['label'])
categorical_labels = to_categorical(encoded_labels)

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(padded_sequences, categorical_labels, test_size=0.2, random_state=42)

# Building the CNN Model with Dropout and Regularization
vocab_size = len(tokenizer.word_index) + 1
model = Sequential([
    Embedding(vocab_size, 100, input_length=max_length),
    Conv1D(128, 5, activation='relu', kernel_regularizer=l2(0.01)),  # Adding L2 regularization
    Dropout(0.5),  # Adding Dropout
    GlobalMaxPooling1D(),
    Dense(10, activation='relu', kernel_regularizer=l2(0.01)),  # Adding L2 regularization
    Dropout(0.5),  # Adding Dropout
    Dense(3, activation='softmax')
])

# Compile the Model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the Model
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the Model
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Accuracy: {accuracy * 100}%')

import nltk
nltk.download('punkt')

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# # Load the dataset
# data = pd.read_csv('your_dataset.csv')

# # Basic preprocessing
# data['article_body'].fillna('', inplace=True)
# Load the CSV file
file_path = '/content/finaldata.csv'  # Replace with your CSV file path
data = pd.read_csv(file_path)
data['article_body'] = data['article_body'].apply(remove_stopwords)

tfidf_vectorizer = TfidfVectorizer(max_features=1000)
X_tfidf = tfidf_vectorizer.fit_transform(data['article_body']).toarray()

# Create Target Variable
# data['price_change'] = (data['open_price_next_day'] - data['open_price_day']) / data['open_price_day']
# threshold = 0.01
# data['price_change_category'] = pd.cut(data['price_change'], bins=[-np.inf, -threshold, threshold, np.inf], labels=['Decrease', 'Neutral', 'Increase'])

# Encoding the target variable
label_encoder = LabelEncoder()
data['label'] = label_encoder.fit_transform(data['label'])
label_names = label_encoder.inverse_transform([0, 1, 2])
# Split the data
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, data['label'], test_size=0.3, random_state=42)

# Train SVM and Random Forest models
svm_model = SVC().fit(X_train, y_train)
svm_linear_model = SVC(kernel='linear').fit(X_train, y_train)
rf_model = RandomForestClassifier().fit(X_train, y_train)

# Predict and evaluate
y_pred_svm = svm_model.predict(X_test)
y_pred_rf = rf_model.predict(X_test)
accuracy_svm = accuracy_score(y_test, y_pred_svm)
accuracy_rf = accuracy_score(y_test, y_pred_rf)

# Confusion Matrix for SVM and Random Forest
cm_svm = confusion_matrix(y_test, y_pred_svm)
cm_rf = confusion_matrix(y_test, y_pred_rf)

# Get feature names from the vectorizer
feature_names = np.array(tfidf_vectorizer.get_feature_names_out())

# For SVM, if the kernel is linear, we can use the coefficients
# For Random Forest, we can use feature importances
svm_coefs = svm_linear_model.coef_
# SVM Feature Importance (for linear kernel)
if isinstance(svm_model, SVC) and svm_model.kernel == 'linear':
    svm_coef = svm_model.coef_
    # Get top features for each class
    top_features_svm = {label: feature_names[coef.argsort()[-10:]] for label, coef in zip(label_names, svm_coef)}

top_features_per_class = {}
for i, class_label in enumerate(label_encoder.classes_):
    top_features_index = svm_coefs[i].argsort()[-10:][::-1]  # Get the indices for the top 10 features
    top_features_names = feature_names[top_features_index]    # Get the names of these features
    top_features_per_class[class_label] = top_features_names  # Map class label to its top features

for class_label, top_features in top_features_per_class.items():
    print(f"Class {class_label}:")
    print(f"{top_features}\n")
# Random Forest Feature Importance
rf_importance = rf_model.feature_importances_
# Get top features
sorted_idx = np.argsort(rf_importance)[-10:]
top_features_rf = feature_names[sorted_idx]

# Display the top features for SVM (if linear) and Random Forest
print(accuracy_svm, accuracy_rf)
print("Top features for each class from SVM (if linear):")
if 'top_features_svm' in locals():
    for label, features in top_features_svm.items():
        print(f"Class {label}: {features}")

print("\nTop features from Random Forest:")
print(top_features_rf)




# plt.tight_layout()
# plt.show()
fig, ax = plt.subplots(1, 2, figsize=(12, 5))
sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Blues', ax=ax[0], xticklabels=label_names, yticklabels=label_names)
ax[0].set_title('SVM Confusion Matrix')
ax[0].set_xlabel('Predicted Labels')
ax[0].set_ylabel('True Labels')

sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', ax=ax[1], xticklabels=label_names, yticklabels=label_names)
ax[1].set_title('Random Forest Confusion Matrix')
ax[1].set_xlabel('Predicted Labels')
ax[1].set_ylabel('True Labels')

plt.tight_layout()
plt.show()
# Comparing Accuracies in a Graph
model_names = ['SVM', 'Random Forest']
accuracies = [accuracy_svm, accuracy_rf]

plt.figure(figsize=(8, 5))
plt.bar(model_names, accuracies, color=['blue', 'green'])
plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.title('Comparison of Model Accuracies')
plt.ylim(0, 1)
plt.show()



import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

from nltk.tokenize import word_tokenize

# This function tokenizes the text, removes stopwords, and rejoins the text
def remove_stopwords(text):
    stop_words = set(stopwords.words('english'))
    word_tokens = word_tokenize(text)
    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]
    return " ".join(filtered_sentence)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer

# Load the CSV file
file_path = '/content/finaldata.csv'  # Replace with your CSV file path
data = pd.read_csv(file_path)

# Extracting features and target
# For article body
X = data['article_body']  # Or use 'headline' for the article title
y = data['label']
model_metrics={}

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score
from sklearn.model_selection import train_test_split

# Preprocess the text
vectorizer = TfidfVectorizer(max_features=5000)
X_vectorized = vectorizer.fit_transform(X)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)

# Train the SVM model
svm_model = SVC()
svm_model.fit(X_train, y_train)

# Evaluate the model
svm_predictions = svm_model.predict(X_test)
svm_accuracy = accuracy_score(y_test, svm_predictions)
recall_svm = recall_score(y_test, svm_predictions, average='macro')
precision_svm = precision_score(y_test, svm_predictions, average='macro')
f1_svm = f1_score(y_test, svm_predictions, average='macro')
model_metrics['SVM'] = {'Accuracy': svm_accuracy, 'Recall': recall_svm, 'Precision': precision_svm, 'F1 Score': f1_svm}

from sklearn.ensemble import RandomForestClassifier

# Train the Random Forest model
rf_model = RandomForestClassifier()
rf_model.fit(X_train, y_train)

# Evaluate the model
rf_predictions = rf_model.predict(X_test)
rf_accuracy = accuracy_score(y_test, rf_predictions)
recall_rf = recall_score(y_test, rf_predictions, average='macro')
precision_rf = precision_score(y_test, rf_predictions, average='macro')
f1_rf = f1_score(y_test, rf_predictions, average='macro')

model_metrics['Random Forest'] = {'Accuracy': rf_accuracy, 'Recall': recall_rf, 'Precision': precision_rf, 'F1 Score': f1_rf}

!pip install tensorflow
from keras.models import Sequential
from keras.layers import Dense, Embedding, Conv1D, GlobalMaxPooling1D
from keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder
import numpy as np


# Tokenize and pad sequences
# tokenizer = Tokenizer(num_words=5000)
# tokenizer.fit_on_texts(X)
# X_seq = tokenizer.texts_to_sequences(X)
# X_pad = pad_sequences(X_seq, maxlen=100)

# # Encode labels
# label_encoder = LabelEncoder()
# y_encoded = label_encoder.fit_transform(y)
# y_cat = to_categorical(y_encoded)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Tokenize and pad the training sequences
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(X_train)
X_train_seq = tokenizer.texts_to_sequences(X_train)
X_train_pad = pad_sequences(X_train_seq, maxlen=100)

# Tokenize and pad the testing sequences
X_test_seq = tokenizer.texts_to_sequences(X_test)
X_test_pad = pad_sequences(X_test_seq, maxlen=100)

# Encode and categorize the labels for both training and testing sets
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.transform(y_test)
y_train_cat = to_categorical(y_train_encoded)
y_test_cat = to_categorical(y_test_encoded)

# Build and train the CNN model
cnn_model = Sequential([
    Embedding(5000, 50, input_length=100),
    Conv1D(128, 5, activation='relu'),
    GlobalMaxPooling1D(),
    Dense(len(label_encoder.classes_), activation='softmax')
])
cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
cnn_model.fit(X_train_pad, y_train_cat, epochs=3, batch_size=32, validation_split=0.2)
cnn_loss, cnn_accuracy = cnn_model.evaluate(X_test_pad, y_test_cat, verbose=0)

# Predict with the CNN Model
y_pred_cnn = cnn_model.predict(X_test_pad)
# Convert predictions to label indices
y_pred_cnn_indices = np.argmax(y_pred_cnn, axis=1)

# Calculate metrics
accuracy_cnn = accuracy_score(y_test_encoded, y_pred_cnn_indices)
recall_cnn = recall_score(y_test_encoded, y_pred_cnn_indices, average='macro')
precision_cnn = precision_score(y_test_encoded, y_pred_cnn_indices, average='macro')
f1_cnn = f1_score(y_test_encoded, y_pred_cnn_indices, average='macro')

# Add the CNN metrics to your comparison structure
model_metrics['CNN'] = {'Accuracy': accuracy_cnn, 'Recall': recall_cnn, 'Precision': precision_cnn, 'F1 Score': f1_cnn}

from transformers import BertTokenizer, TFBertForSequenceClassification
import numpy as np
from sklearn.model_selection import train_test_split

# Initialize the tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Tokenize the dataset
X_tokenized = tokenizer.batch_encode_plus(X.tolist(), padding=True, truncation=True, max_length=512, return_tensors='np')

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X_tokenized.data, y_encoded, test_size=0.2, random_state=42)

# Prepare input for BERT
X_train_input = {key: X_train[key] for key in X_tokenized.data.keys()}
X_test_input = {key: X_test[key] for key in X_tokenized.data.keys()}

# Build and train the BERT model
bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))
bert_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
bert_model.fit(X_train_input, y_train, epochs=3, batch_size=32, validation_split=0.2)
bert_loss, bert_accuracy = bert_model.evaluate(X_test_input, y_test, verbose=0)

# Print BERT model accuracy
print(f"BERT Model Accuracy: {bert_accuracy}")

import nltk
nltk.download('vader_lexicon')

from nltk.sentiment import SentimentIntensityAnalyzer
from sklearn.metrics import accuracy_score

# Initialize VADER
sia = SentimentIntensityAnalyzer()

label_encoder = LabelEncoder()
label_encoder.fit(data['label'])
encoded_labels = label_encoder.transform(data['label'])

# Function to classify sentiment
def classify_sentiment(text):
    score = sia.polarity_scores(text)['compound']
    if score > 0.05:
        return 'I'  # Positive sentiment
    elif score < -0.05:
        return 'D'  # Negative sentiment
    else:
        return 'N'   # Neutral sentiment

# Apply the function to your text data
# predicted_sentiments = data['article_body'].apply(classify_sentiment)

# # Calculate accuracy
# accuracy = accuracy_score(data['label'], predicted_sentiments)
# print(f"Accuracy: {accuracy}")

# predicted_sentiments_encoded = label_encoder.transform(predicted_sentiments)

# # Calculate metrics
# accuracy_vader = accuracy_score(data['label'], predicted_sentiments_encoded)
# recall_vader = recall_score(data['label'], predicted_sentiments_encoded, average='macro')
# precision_vader = precision_score(data['label'], predicted_sentiments_encoded, average='macro')
# f1_vader = f1_score(data['label'], predicted_sentiments_encoded, average='macro')

# # Add the VADER metrics to your comparison structure
# model_metrics['SIA'] = {'Accuracy': accuracy_vader, 'Recall': recall_vader, 'Precision': precision_vader, 'F1 Score': f1_vader}

predicted_sentiments = data['article_body'].apply(classify_sentiment)
predicted_sentiments_encoded = label_encoder.transform(predicted_sentiments)

# Calculate metrics
accuracy_vader = accuracy_score(encoded_labels, predicted_sentiments_encoded)
recall_vader = recall_score(encoded_labels, predicted_sentiments_encoded, average='macro')
precision_vader = precision_score(encoded_labels, predicted_sentiments_encoded, average='macro')
f1_vader = f1_score(encoded_labels, predicted_sentiments_encoded, average='macro')

# Add the VADER metrics to your comparison structure
model_metrics['SIA'] = {'Accuracy': accuracy_vader, 'Recall': recall_vader, 'Precision': precision_vader, 'F1 Score': f1_vader}

import matplotlib.pyplot as plt

# # Accuracies obtained from the models
# accuracies = {
#     'SVM': svm_accuracy,         # Replace with the actual accuracy from the SVM model
#     'Random Forest': rf_accuracy, # Replace with the actual accuracy from the RF model
#     'CNN': cnn_accuracy,         # Replace with the actual accuracy from the CNN model
#     'NLTK SIA': accuracy,        # Replace with the actual accuracy from the BERT model
# }

# print(accuracies)

# # Plotting the graph
# plt.bar(accuracies.keys(), accuracies.values())
# plt.xlabel('Model')
# plt.ylabel('Accuracy')
# plt.title('Comparison of Model Accuracies')
# plt.ylim([0, 1])  # Assuming accuracy values are between 0 and 1
# plt.show()

model_metrics["BERT"] = {'Accuracy': 0.3947, 'Recall':0.3947, 'Precision':0.15575 , 'F1 Score':0.22335}
metrics = ['Accuracy', 'Recall', 'Precision', 'F1 Score']
fig, axs = plt.subplots(1, len(metrics), figsize=(20, 5))

for i, metric in enumerate(metrics):
    values = [model_metrics[model][metric] for model in model_metrics]
    axs[i].bar(model_metrics.keys(), values)
    axs[i].set_title(metric)
    axs[i].set_ylim(0, 1)

plt.tight_layout()
plt.show()

BertEPochs = {1:0.3956,2:0.3848,3:0.3875}